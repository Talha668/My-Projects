# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11s_pOs-o_K7LBxXKwna4VunNAbotqyxY
"""

"""
LSTM AI Model trained directly from API URLs
Fetches data from your APIs and trains the model
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import os
import json
from typing import List, Tuple, Dict, Optional, Union
import re
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import hashlib

class APIDataFetcher:
    """
    Fetches and processes data from API URLs
    """

    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LSTM-Model-Trainer/1.0',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        })

    def fetch_from_api(self, api_url: str, params: dict = None,
                       headers: dict = None, method: str = 'GET',
                       data: dict = None, timeout: int = 30) -> Optional[dict]:
        """
        Fetch data from a single API endpoint
        """
        try:
            # Update headers if provided
            request_headers = self.session.headers.copy()
            if headers:
                request_headers.update(headers)

            if method.upper() == 'GET':
                response = self.session.get(
                    api_url,
                    params=params,
                    headers=request_headers,
                    timeout=timeout
                )
            elif method.upper() == 'POST':
                response = self.session.post(
                    api_url,
                    json=data,
                    params=params,
                    headers=request_headers,
                    timeout=timeout
                )
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            response.raise_for_status()

            # Try to parse JSON
            try:
                return response.json()
            except:
                # If not JSON, return as text
                return {"text": response.text}

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {api_url}: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error with {api_url}: {e}")
            return None

    def extract_text_from_response(self, response_data: Union[dict, list, str]) -> List[str]:
        """
        Extract text from various API response formats
        """
        texts = []

        def extract(obj, current_path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    # Look for text-like fields
                    if isinstance(value, str) and len(value.strip()) > 10:
                        # Check if field name suggests text content
                        text_fields = ['text', 'content', 'message', 'description',
                                      'body', 'title', 'name', 'summary', 'response',
                                      'answer', 'comment', 'review', 'feedback',
                                      'prompt', 'query', 'question']

                        if any(text_field in key.lower() for text_field in text_fields):
                            texts.append(value.strip())
                        elif len(value.split()) > 3:  # Heuristic: has multiple words
                            texts.append(value.strip())

                    elif isinstance(value, (dict, list)):
                        extract(value, f"{current_path}.{key}")

            elif isinstance(obj, list):
                for item in obj:
                    extract(item, current_path)

        extract(response_data)
        return texts

    def fetch_multiple_apis(self, api_configs: List[dict]) -> List[str]:
        """
        Fetch data from multiple API URLs concurrently
        """
        all_texts = []

        print(f"Fetching data from {len(api_configs)} API endpoints...")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all API requests
            future_to_api = {}
            for config in api_configs:
                future = executor.submit(
                    self.fetch_from_api,
                    api_url=config.get('url'),
                    params=config.get('params'),
                    headers=config.get('headers'),
                    method=config.get('method', 'GET'),
                    data=config.get('data'),
                    timeout=config.get('timeout', 30)
                )
                future_to_api[future] = config.get('url')

            # Process completed requests
            for future in as_completed(future_to_api):
                api_url = future_to_api[future]
                try:
                    data = future.result()
                    if data:
                        extracted_texts = self.extract_text_from_response(data)
                        all_texts.extend(extracted_texts)
                        print(f"✓ {api_url}: Extracted {len(extracted_texts)} text samples")
                    else:
                        print(f"✗ {api_url}: No data received")
                except Exception as e:
                    print(f"✗ {api_url}: Error - {e}")

                # Be polite to APIs
                time.sleep(0.5)

        print(f"\nTotal text samples extracted: {len(all_texts)}")
        return all_texts


class LSTMModelTrainer:
    """
    Train and use LSTM model on API data
    """

    def __init__(self, model_name: str = "api_lstm_model"):
        self.model_name = model_name
        self.tokenizer = Tokenizer(oov_token="<OOV>", filters='')
        self.model = None
        self.max_sequence_len = None
        self.vocab_size = None

    def preprocess_text(self, texts: List[str]) -> List[str]:
        """
        Clean and prepare text data
        """
        processed_texts = []

        for text in texts:
            if not isinstance(text, str):
                continue

            # Basic cleaning
            text = text.lower()

            # Remove URLs
            text = re.sub(r'https?://\S+|www\.\S+', '', text)

            # Remove special characters but keep basic punctuation
            text = re.sub(r'[^\w\s.,!?;:\'\"-]', '', text)

            # Remove extra whitespace
            text = re.sub(r'\s+', ' ', text).strip()

            # Remove very short texts
            if len(text) > 20 and len(text.split()) > 3:
                processed_texts.append(text)

        # Remove duplicates while preserving order
        seen = set()
        unique_texts = []
        for text in processed_texts:
            text_hash = hashlib.md5(text.encode()).hexdigest()
            if text_hash not in seen:
                seen.add(text_hash)
                unique_texts.append(text)

        print(f"After preprocessing: {len(unique_texts)} unique texts")
        return unique_texts

    def prepare_sequences(self, texts: List[str], seq_length: int = 50) -> Tuple[np.ndarray, np.ndarray]:
        """
        Convert text to training sequences
        """
        if not texts:
            raise ValueError("No text data available for training")

        print(f"\nPreparing sequences with max length: {seq_length}")

        # Fit tokenizer on all texts
        self.tokenizer.fit_on_texts(texts)
        self.vocab_size = len(self.tokenizer.word_index) + 1
        print(f"Vocabulary size: {self.vocab_size}")

        # Create sequences
        sequences = []
        for text in texts:
            token_list = self.tokenizer.texts_to_sequences([text])[0]

            # Generate sequences
            for i in range(seq_length, len(token_list)):
                n_gram_sequence = token_list[i-seq_length:i+1]
                sequences.append(n_gram_sequence)

        if not sequences:
            # Fallback: create shorter sequences if needed
            for text in texts:
                token_list = self.tokenizer.texts_to_sequences([text])[0]
                if len(token_list) >= 10:
                    for i in range(10, len(token_list)):
                        n_gram_sequence = token_list[i-10:i+1]
                        sequences.append(n_gram_sequence)

        if not sequences:
            raise ValueError("Could not create sequences. Texts might be too short.")

        # Find max sequence length
        self.max_sequence_len = max([len(seq) for seq in sequences])
        print(f"Actual sequence length: {self.max_sequence_len}")

        # Pad sequences
        sequences = np.array(pad_sequences(
            sequences,
            maxlen=self.max_sequence_len,
            padding='pre'
        ))

        # Split into input (X) and output (y)
        X = sequences[:, :-1]
        y = sequences[:, -1]

        # Convert y to one-hot encoding
        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)

        print(f"Created {len(X)} training sequences")
        return X, y

    def build_lstm_model(self, embedding_dim: int = 128,
                         lstm_units: int = 256,
                         dropout_rate: float = 0.3) -> tf.keras.Model:
        """
        Build the LSTM model architecture
        """
        model = Sequential([
            # Embedding layer
            Embedding(
                input_dim=self.vocab_size,
                output_dim=embedding_dim,
                input_length=self.max_sequence_len - 1
            ),

            # First LSTM layer
            LSTM(lstm_units, return_sequences=True),
            Dropout(dropout_rate),

            # Second LSTM layer
            LSTM(lstm_units // 2),
            Dropout(dropout_rate),

            # Dense layers
            Dense(lstm_units // 4, activation='relu'),
            Dropout(dropout_rate),

            # Output layer
            Dense(self.vocab_size, activation='softmax')
        ])

        # Compile model
        model.compile(
            loss='categorical_crossentropy',
            optimizer=Adam(learning_rate=0.001),
            metrics=['accuracy']
        )

        print("\nModel Architecture:")
        model.summary()
        self.model = model
        return model

    def train(self, X: np.ndarray, y: np.ndarray,
              epochs: int = 50,
              batch_size: int = 128,
              validation_split: float = 0.1) -> dict:
        """
        Train the model
        """
        print(f"\nStarting training...")
        print(f"Training samples: {len(X)}")
        print(f"Batch size: {batch_size}")
        print(f"Epochs: {epochs}")

        # Create callbacks
        callbacks = [
            ModelCheckpoint(
                f'{self.model_name}_best.h5',
                monitor='val_accuracy',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            )
        ]

        # Train
        history = self.model.fit(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )

        print("\nTraining completed!")
        return history.history

    def save_model(self, save_dir: str = "./trained_models"):
        """
        Save the trained model and tokenizer
        """
        os.makedirs(save_dir, exist_ok=True)

        # Save model
        model_path = os.path.join(save_dir, f"{self.model_name}.h5")
        self.model.save(model_path)

        # Save tokenizer
        tokenizer_path = os.path.join(save_dir, f"{self.model_name}_tokenizer.pkl")
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)

        # Save metadata
        metadata = {
            'vocab_size': self.vocab_size,
            'max_sequence_len': self.max_sequence_len,
            'model_name': self.model_name,
            'word_index': self.tokenizer.word_index
        }

        metadata_path = os.path.join(save_dir, f"{self.model_name}_metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"\nModel saved successfully!")
        print(f"Model: {model_path}")
        print(f"Tokenizer: {tokenizer_path}")
        print(f"Metadata: {metadata_path}")

    def load_model(self, model_dir: str = "./trained_models"):
        """
        Load a saved model
        """
        model_path = os.path.join(model_dir, f"{self.model_name}.h5")
        tokenizer_path = os.path.join(model_dir, f"{self.model_name}_tokenizer.pkl")
        metadata_path = os.path.join(model_dir, f"{self.model_name}_metadata.json")

        # Check if files exist
        for path in [model_path, tokenizer_path, metadata_path]:
            if not os.path.exists(path):
                raise FileNotFoundError(f"File not found: {path}")

        # Load model
        self.model = load_model(model_path)

        # Load tokenizer
        with open(tokenizer_path, 'rb') as f:
            self.tokenizer = pickle.load(f)

        # Load metadata
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
            self.vocab_size = metadata['vocab_size']
            self.max_sequence_len = metadata['max_sequence_len']

        print(f"Model loaded from {model_path}")
        print(f"Vocabulary size: {self.vocab_size}")

    def generate_text(self, seed_text: str,
                      num_words: int = 100,
                      temperature: float = 0.7,
                      max_attempts: int = 3) -> str:
        """
        Generate text from seed text
        """
        if self.model is None:
            raise ValueError("Model not loaded. Train or load a model first.")

        generated_text = seed_text

        for word_num in range(num_words):
            for attempt in range(max_attempts):
                try:
                    # Tokenize and pad
                    token_list = self.tokenizer.texts_to_sequences([seed_text])[0]
                    token_list = pad_sequences(
                        [token_list],
                        maxlen=self.max_sequence_len - 1,
                        padding='pre'
                    )

                    # Predict
                    predictions = self.model.predict(token_list, verbose=0)[0]

                    # Apply temperature
                    if temperature > 0:
                        predictions = np.log(predictions + 1e-10) / temperature
                        exp_predictions = np.exp(predictions)
                        predictions = exp_predictions / np.sum(exp_predictions)

                    # Sample next word
                    predicted_id = np.random.choice(len(predictions), p=predictions)

                    # Convert to word
                    output_word = ""
                    for word, idx in self.tokenizer.word_index.items():
                        if idx == predicted_id:
                            output_word = word
                            break

                    # Update texts
                    seed_text += " " + output_word
                    generated_text += " " + output_word
                    break

                except Exception as e:
                    if attempt == max_attempts - 1:
                        print(f"Failed to generate word {word_num + 1}: {e}")
                        return generated_text

        return generated_text

    def get_response(self, prompt: str,
                     max_length: int = 150,
                     temperature: float = 0.7) -> str:
        """
        Get AI response for a prompt
        """
        # Clean prompt
        prompt = prompt.lower().strip()
        prompt = re.sub(r'[^\w\s.,!?]', '', prompt)

        # Generate response
        full_response = self.generate_text(
            seed_text=prompt,
            num_words=max_length,
            temperature=temperature
        )

        # Extract only the generated part
        response = full_response[len(prompt):].strip()

        # Format response
        if response:
            # Capitalize first letter
            response = response[0].upper() + response[1:]

            # Ensure proper ending
            if response and response[-1] not in '.!?':
                response += '.'

            # Limit length
            if len(response.split()) > max_length:
                response = ' '.join(response.split()[:max_length]) + '...'

        return response


class APIAIModel:
    """
    Main class to manage API data fetching and model training
    """

    def __init__(self):
        self.data_fetcher = APIDataFetcher(max_workers=5)
        self.model_trainer = None
        self.api_configs = []
        self.training_data = []

    def add_api_endpoint(self, url: str, method: str = 'GET',
                         params: dict = None, headers: dict = None,
                         data: dict = None, name: str = None):
        """
        Add an API endpoint to fetch data from
        """
        if not name:
            # Generate name from URL
            parsed = urlparse(url)
            name = f"{parsed.netloc}{parsed.path.replace('/', '_')}"

        api_config = {
            'name': name,
            'url': url,
            'method': method,
            'params': params or {},
            'headers': headers or {},
            'data': data,
            'timeout': 30
        }

        self.api_configs.append(api_config)
        print(f"Added API: {name} - {url}")

    def add_multiple_apis(self, apis: List[dict]):
        """
        Add multiple APIs at once
        """
        for api in apis:
            self.add_api_endpoint(**api)

    def fetch_data_from_apis(self) -> List[str]:
        """
        Fetch data from all added APIs
        """
        if not self.api_configs:
            print("No APIs added. Please add API endpoints first.")
            return []

        print(f"\nFetching data from {len(self.api_configs)} APIs...")
        all_texts = self.data_fetcher.fetch_multiple_apis(self.api_configs)

        # Store for later use
        self.training_data = all_texts

        # Save fetched data for reference
        if all_texts:
            os.makedirs("./api_data", exist_ok=True)
            with open("./api_data/fetched_texts.json", 'w', encoding='utf-8') as f:
                json.dump(all_texts, f, indent=2, ensure_ascii=False)
            print(f"Fetched data saved to ./api_data/fetched_texts.json")

        return all_texts

    def train_model(self, model_name: str = "api_trained_model",
                    seq_length: int = 50, epochs: int = 50,
                    batch_size: int = 64, test_size: float = 0.1):
        """
        Train LSTM model on fetched API data
        """
        if not self.training_data:
            print("No training data available. Please fetch data from APIs first.")
            return False

        print("\n" + "="*60)
        print("TRAINING LSTM MODEL ON API DATA")
        print("="*60)

        # Initialize trainer
        self.model_trainer = LSTMModelTrainer(model_name=model_name)

        # Preprocess data
        print("\n1. Preprocessing text data...")
        processed_texts = self.model_trainer.preprocess_text(self.training_data)

        if not processed_texts:
            print("No valid text data after preprocessing.")
            return False

        # Prepare sequences
        print("\n2. Preparing training sequences...")
        try:
            X, y = self.model_trainer.prepare_sequences(processed_texts, seq_length)
        except ValueError as e:
            print(f"Error preparing sequences: {e}")
            return False

        # Build model
        print("\n3. Building LSTM model...")
        self.model_trainer.build_lstm_model(
            embedding_dim=128,
            lstm_units=256,
            dropout_rate=0.3
        )

        # Train model
        print("\n4. Training model...")
        history = self.model_trainer.train(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=test_size
        )

        # Save model
        print("\n5. Saving trained model...")
        self.model_trainer.save_model()

        # Display results
        print("\n" + "="*60)
        print("TRAINING COMPLETED!")
        print("="*60)
        print(f"Final Validation Accuracy: {history.get('val_accuracy', [0])[-1]:.2%}")
        print(f"Final Validation Loss: {history.get('val_loss', [0])[-1]:.4f}")

        return True

    def load_trained_model(self, model_name: str = "api_trained_model"):
        """
        Load a previously trained model
        """
        try:
            self.model_trainer = LSTMModelTrainer(model_name=model_name)
            self.model_trainer.load_model()
            print(f"Model '{model_name}' loaded successfully!")
            return True
        except Exception as e:
            print(f"Error loading model: {e}")
            return False

    def test_model(self, test_prompts: List[str] = None):
        """
        Test the trained model
        """
        if self.model_trainer is None or self.model_trainer.model is None:
            print("Model not loaded. Please train or load a model first.")
            return

        if test_prompts is None:
            # Sample prompts based on API data
            test_prompts = [
                "What is this project about?",
                "how many phases of dha are there?",
                "What kind of property is for sale?",
                "what is down payment?",
                "Explain the mechanism of this project?",
            ]

        print("\n" + "="*60)
        print("MODEL TESTING")
        print("="*60)

        for i, prompt in enumerate(test_prompts, 1):
            response = self.model_trainer.get_response(prompt, max_length=100)
            print(f"\n{i}. Prompt: {prompt}")
            print(f"   Response: {response}")
            print("-"*50)

    def interactive_chat(self):
        """
        Interactive chat with the AI model
        """
        if self.model_trainer is None or self.model_trainer.model is None:
            print("Model not loaded. Please train or load a model first.")
            return

        print("\n" + "="*60)
        print("AI CHAT INTERFACE")
        print("="*60)
        print("Ask questions about your APIs/data")
        print("Commands:")
        print("  'quit' or 'exit' - End chat")
        print("  'temp X' - Set temperature (0.1-1.0, default: 0.7)")
        print("  'len X' - Set response length (default: 100)")
        print("="*60)

        temperature = 0.7
        response_length = 100

        while True:
            try:
                user_input = input("\nYou: ").strip()

                if not user_input:
                    continue

                # Check for commands
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("Goodbye!")
                    break

                elif user_input.lower().startswith('temp '):
                    try:
                        new_temp = float(user_input.split()[1])
                        if 0.1 <= new_temp <= 1.0:
                            temperature = new_temp
                            print(f"Temperature set to {temperature}")
                        else:
                            print("Temperature must be between 0.1 and 1.0")
                    except:
                        print("Invalid format. Use: temp 0.7")
                    continue

                elif user_input.lower().startswith('len '):
                    try:
                        new_len = int(user_input.split()[1])
                        if 10 <= new_len <= 500:
                            response_length = new_len
                            print(f"Response length set to {response_length}")
                        else:
                            print("Length must be between 10 and 500")
                    except:
                        print("Invalid format. Use: len 100")
                    continue

                # Get AI response
                start_time = time.time()
                response = self.model_trainer.get_response(
                    prompt=user_input,
                    max_length=response_length,
                    temperature=temperature
                )
                response_time = time.time() - start_time

                print(f"AI: {response}")
                print(f"[Generated in {response_time:.2f}s, temp={temperature}]")

            except KeyboardInterrupt:
                print("\n\nChat ended by user.")
                break
            except Exception as e:
                print(f"Error: {e}")


def create_api_configs_from_urls(api_urls: List[str]) -> List[dict]:
    """
    Helper function to create API configs from simple URLs
    """
    configs = []
    for i, url in enumerate(api_urls):
        configs.append({
            'url': url,
            'method': 'GET',
            'name': f'api_{i+1}'
        })
    return configs


def main():
    """
    Main execution function
    """
    print("="*70)
    print("LSTM AI MODEL TRAINED ON API DATA")
    print("="*70)

    # Initialize the system
    ai_system = APIAIModel()

    while True:
        print("\n" + "="*70)
        print("MAIN MENU")
        print("="*70)
        print("1. Add API endpoints")
        print("2. Fetch data from APIs")
        print("3. Train new model")
        print("4. Load existing model")
        print("5. Test model")
        print("6. Interactive chat")
        print("7. View training data")
        print("8. Exit")

        choice = input("\nEnter choice (1-8): ").strip()

        if choice == '1':
            print("\nAdd API Endpoints")
            print("-" * 40)
            print("Enter API URLs (one per line, empty line to finish):")

            api_urls = []
            while True:
                url = input("API URL: ").strip()
                if not url:
                    break
                if url.startswith('http'):
                    api_urls.append(url)
                    print(f"Added: {url}")
                else:
                    print("Invalid URL. Must start with http:// or https://")

            if api_urls:
                configs = create_api_configs_from_urls(api_urls)
                ai_system.add_multiple_apis(configs)
                print(f"\nAdded {len(api_urls)} API endpoints.")

        elif choice == '2':
            if not ai_system.api_configs:
                print("No APIs added. Please add API endpoints first.")
            else:
                print(f"\nFetching data from {len(ai_system.api_configs)} APIs...")
                ai_system.fetch_data_from_apis()

        elif choice == '3':
            if not ai_system.training_data:
                print("No training data. Please fetch data from APIs first.")
                continue

            print("\nTrain New Model")
            print("-" * 40)

            model_name = input("Model name [api_model]: ").strip() or "api_model"
            seq_length = input("Sequence length [50]: ").strip()
            seq_length = int(seq_length) if seq_length else 50
            epochs = input("Training epochs [30]: ").strip()
            epochs = int(epochs) if epochs else 30
            batch_size = input("Batch size [64]: ").strip()
            batch_size = int(batch_size) if batch_size else 64

            success = ai_system.train_model(
                model_name=model_name,
                seq_length=seq_length,
                epochs=epochs,
                batch_size=batch_size
            )

            if success:
                print("\nModel trained successfully!")

        elif choice == '4':
            model_name = input("Enter model name to load [api_trained_model]: ").strip()
            model_name = model_name or "api_trained_model"

            if ai_system.load_trained_model(model_name):
                print("Model loaded successfully!")

        elif choice == '5':
            ai_system.test_model()

        elif choice == '6':
            ai_system.interactive_chat()

        elif choice == '7':
            if ai_system.training_data:
                print(f"\nTraining Data Summary:")
                print(f"Total text samples: {len(ai_system.training_data)}")
                print(f"\nSample texts:")
                for i, text in enumerate(ai_system.training_data[:5], 1):
                    preview = text[:100] + "..." if len(text) > 100 else text
                    print(f"{i}. {preview}")
            else:
                print("No training data available.")

        elif choice == '8':
            print("\nThank you for using the API AI Model!")
            break

        else:
            print("Invalid choice. Please enter 1-8.")


def quick_start_example():
    """
    Quick start with example API URLs
    Replace these with your actual API URLs
    """
    print("\n" + "="*70)
    print("QUICK START EXAMPLE")
    print("="*70)

    # YOUR API URLs GO HERE - REPLACE WITH YOUR ACTUAL APIS
    your_api_urls = [
        # Add your actual API URLs here:
         "https://api.dhaconnects.com/api/property",
    ]

    print("\nUsing example APIs. Replace with your actual API URLs in the code.")
    print(f"Number of APIs: {len(your_api_urls)}")

    # Create the AI system
    ai_system = APIAIModel()

    # Add APIs
    configs = create_api_configs_from_urls(your_api_urls)
    ai_system.add_multiple_apis(configs)

    # Fetch data
    print("\nFetching data from APIs...")
    ai_system.fetch_data_from_apis()

    if not ai_system.training_data:
        print("No data fetched. Check your API URLs and internet connection.")
        return

    # Train model
    print("\nTraining model...")
    success = ai_system.train_model(
        model_name="my_api_ai",
        seq_length=50,
        epochs=30,
        batch_size=64
    )

    if success:
        # Test the model
        ai_system.test_model()

        # Optional: Start interactive chat
        # ai_system.interactive_chat()


if __name__ == "__main__":
    # Run the main program
    main()

    # Or run quick start (comment out main() above first)
    # quick_start_example()