# -*- coding: utf-8 -*-
"""Improved LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nnzbnsn5tFPO7q7lvoYBASmWdQb6oF3Z
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import os
import json
from typing import List, Tuple, Dict, Optional, Union
import re
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import hashlib
import random
from collections import defaultdict

class APIDataFetcher:
    """
    Fetches and processes data from API URLs
    """

    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'LSTM-Model-Trainer/1.0',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        })

    def fetch_from_api(self, api_url: str, params: dict = None,
                       headers: dict = None, method: str = 'GET',
                       data: dict = None, timeout: int = 30) -> Optional[dict]:
        """
        Fetch data from a single API endpoint
        """
        try:
            # Update headers if provided
            request_headers = self.session.headers.copy()
            if headers:
                request_headers.update(headers)

            if method.upper() == 'GET':
                response = self.session.get(
                    api_url,
                    params=params,
                    headers=request_headers,
                    timeout=timeout
                )
            elif method.upper() == 'POST':
                response = self.session.post(
                    api_url,
                    json=data,
                    params=params,
                    headers=request_headers,
                    timeout=timeout
                )
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            response.raise_for_status()

            # Try to parse JSON
            try:
                return response.json()
            except:
                # If not JSON, return as text
                return {"text": response.text}

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {api_url}: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error with {api_url}: {e}")
            return None

    def extract_structured_data(self, response_data: Union[dict, list, str]) -> List[dict]:
        """
        Extract structured data from API responses for paragraph creation
        """
        structured_items = []

        def extract_items(obj, path=""):
            if isinstance(obj, dict):
                # Check if this looks like a property/item object
                has_property_fields = any(key in obj for key in ['id', 'price', 'location', 'property_type', 'bedrooms', 'area'])

                if has_property_fields and len(obj) >= 3:  # At least 3 relevant fields
                    structured_items.append(obj)
                else:
                    for key, value in obj.items():
                        extract_items(value, f"{path}.{key}")

            elif isinstance(obj, list):
                for item in obj:
                    extract_items(item, path)

        extract_items(response_data)
        return structured_items

    def fetch_multiple_apis(self, api_configs: List[dict]) -> List[dict]:
        """
        Fetch data from multiple API URLs concurrently
        """
        all_structured_data = []

        print(f"Fetching data from {len(api_configs)} API endpoints...")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all API requests
            future_to_api = {}
            for config in api_configs:
                future = executor.submit(
                    self.fetch_from_api,
                    api_url=config.get('url'),
                    params=config.get('params'),
                    headers=config.get('headers'),
                    method=config.get('method', 'GET'),
                    data=config.get('data'),
                    timeout=config.get('timeout', 30)
                )
                future_to_api[future] = config.get('url')

            # Process completed requests
            for future in as_completed(future_to_api):
                api_url = future_to_api[future]
                try:
                    data = future.result()
                    if data:
                        # Extract structured data for paragraph creation
                        structured_items = self.extract_structured_data(data)
                        all_structured_data.extend(structured_items)
                        print(f"✓ {api_url}: Extracted {len(structured_items)} structured items")
                    else:
                        print(f"✗ {api_url}: No data received")
                except Exception as e:
                    print(f"✗ {api_url}: Error - {e}")

                # Be polite to APIs
                time.sleep(0.5)

        print(f"\nTotal structured items extracted: {len(all_structured_data)}")
        return all_structured_data


class ParagraphCreator:
    """
    Transforms structured API data into coherent paragraphs
    """

    def __init__(self):
        self.property_templates = [
            # Full property description templates
            "This {property_type} property is located in {location}. It features {bedrooms} bedrooms and covers an area of {area} square yards. The property is priced at {price}, making it an attractive investment opportunity. {location} is known for its excellent amenities and secure environment.",

            "Available for sale is a {property_type} property situated in {location}. With {bedrooms} bedrooms and {area} square yards of space, this property offers comfortable living. The asking price is {price}. The {location} area is highly sought after due to its prime location and quality infrastructure.",

            "A {property_type} property in {location} is currently on the market. This property comprises {bedrooms} bedrooms and spans {area} square yards. Priced at {price}, it represents good value for money. {location} offers excellent connectivity and modern facilities for residents.",

            "This listing features a {property_type} property in the desirable {location} neighborhood. The property includes {bedrooms} bedrooms and has a total area of {area} square yards. The price is set at {price}. Properties in {location} are known for their high appreciation value and quality construction.",

            "For sale: {property_type} property in {location}. This well-maintained property has {bedrooms} bedrooms and covers {area} square yards. The price is {price}. {location} is a premium residential area with excellent security, parks, and community facilities."
        ]

        self.qa_templates = [
            # Q&A paragraph templates
            ("What is the property rate at {location}?",
             "The property rate at {location} is approximately {price} per square yard. This pricing is competitive for the area, considering the amenities and location advantages. Properties in {location} typically range based on phase and property type, with {property_type} properties like this one offering good value."),

            ("How much does a property cost in {location}?",
             "In {location}, properties are priced around {price} for a {property_type} unit with {bedrooms} bedrooms. The exact price can vary based on specific location within {location}, property condition, and exact size. This particular property of {area} square yards is priced at {price}."),

            ("What type of properties are available in {location}?",
             "{location} offers various property types including {property_type} properties. This particular listing is for a {property_type} property with {bedrooms} bedrooms spanning {area} square yards. The property is priced at {price}, which is standard for this type of property in the area."),

            ("Tell me about properties in {location}.",
             "{location} is a prime residential area known for its well-planned infrastructure and security. Properties here range from residential to commercial, with this particular {property_type} property featuring {bedrooms} bedrooms and {area} square yards of space. The current price is {price}, reflecting the area's premium status."),

            ("What are the features of this property?",
             "This property in {location} is a {property_type} unit with {bedrooms} bedrooms covering {area} square yards. Key features include its location in the desirable {location} area, competitive pricing at {price}, and the property type which is ideal for {use_case}. The area offers excellent amenities and security features.")
        ]

        self.paragraph_starters = [
            "In the current real estate market,",
            "This property listing showcases",
            "Available for prospective buyers,",
            "The real estate sector in this area features",
            "This attractive property opportunity includes",
            "Potential investors should consider",
            "This well-positioned property offers",
            "The housing market presents",
            "This premium property features",
            "Real estate enthusiasts will appreciate"
        ]

        self.paragraph_connectors = [
            "Furthermore,",
            "Additionally,",
            "Moreover,",
            "It is also important to note that",
            "Another key aspect is",
            "In addition to these features,",
            "The property also benefits from",
            "Another advantage is",
            "This is complemented by",
            "Adding to its appeal,"
        ]

        self.paragraph_endings = [
            "This property represents excellent value for money.",
            "Early viewing is recommended due to high demand.",
            "This is an ideal opportunity for both investors and end-users.",
            "The property promises good returns on investment.",
            "Serious buyers should consider this attractive offer.",
            "This listing is expected to generate significant interest.",
            "The property meets all modern living requirements.",
            "This represents a sound real estate investment.",
            "The property offers both comfort and investment potential.",
            "This is a prime property in a sought-after location."
        ]

    def create_property_paragraph(self, property_data: dict) -> str:
        """
        Create a coherent paragraph from property data
        """
        # Extract and clean data
        location = property_data.get('location', 'DHA').title()
        property_type = property_data.get('property_type', 'residential').lower()
        bedrooms = property_data.get('bedrooms', '3')
        area = property_data.get('area', '500')
        price = property_data.get('price', 'Not specified')

        # Clean price formatting
        if isinstance(price, (int, float)):
            price = f"PKR {price:,.0f}"
        elif isinstance(price, str) and price.replace('.', '', 1).isdigit():
            try:
                price_numeric = float(price)
                price = f"PKR {price_numeric:,.0f}"
            except ValueError:
                price = str(price) # Fallback if float conversion fails unexpectedly
        else:
            price = str(price) # Ensure it's a string for non-numeric cases

        # Clean area formatting
        if isinstance(area, (int, float)) or (isinstance(area, str) and str(area).replace('.', '', 1).isdigit()):
            try:
                area_numeric = float(area)
                area = f"{area_numeric:,.0f}"
            except ValueError:
                area = f"{area}"
        else:
            area = f"{area}"

        # Select a random template
        template = random.choice(self.property_templates)

        # Fill template
        paragraph = template.format(
            property_type=property_type,
            location=location,
            bedrooms=bedrooms,
            area=area,
            price=price
        )

        # Ensure proper sentence structure
        sentences = paragraph.split('. ')
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if sent:
                # Capitalize first letter
                sent = sent[0].upper() + sent[1:] if sent else sent
                cleaned_sentences.append(sent)

        # Join with proper punctuation
        paragraph = '. '.join(cleaned_sentences) + '.'

        return paragraph

    def create_qa_paragraph(self, property_data: dict) -> List[str]:
        """
        Create Q&A style paragraphs from property data
        """
        qa_paragraphs = []

        # Extract data
        location = property_data.get('location', 'DHA').title()
        property_type = property_data.get('property_type', 'residential').lower()
        bedrooms = property_data.get('bedrooms', '3')
        area = property_data.get('area', '500')
        price = property_data.get('price', 'Not specified')

        # Clean price formatting
        if isinstance(price, (int, float)):
            price = f"PKR {price:,.0f}"
        elif isinstance(price, str) and price.replace('.', '', 1).isdigit():
            try:
                price_numeric = float(price)
                price = f"PKR {price_numeric:,.0f}"
            except ValueError:
                price = str(price) # Fallback if float conversion fails unexpectedly
        else:
            price = str(price) # Ensure it's a string for non-numeric cases

        # Create QA pairs
        for question_template, answer_template in self.qa_templates:
            try:
                question = question_template.format(location=location)
                answer = answer_template.format(
                    location=location,
                    property_type=property_type,
                    bedrooms=bedrooms,
                    area=area,
                    price=price,
                    use_case="family living" if property_type == "residential" else "commercial use"
                )

                # Combine into paragraph
                qa_paragraph = f"{question} {answer}"
                qa_paragraphs.append(qa_paragraph)
            except:
                continue

        return qa_paragraphs

    def create_comprehensive_paragraph(self, property_data: dict) -> str:
        """
        Create a comprehensive, multi-sentence paragraph
        """
        location = property_data.get('location', 'DHA').title()
        property_type = property_data.get('property_type', 'residential').lower()
        bedrooms = property_data.get('bedrooms', '3')
        area = property_data.get('area', '500')
        price = property_data.get('price', 'Not specified')

        # Clean formatting
        if isinstance(price, (int, float)):
            price = f"PKR {price:,.0f}"
        elif isinstance(price, str) and price.replace('.', '', 1).isdigit():
            try:
                price_numeric = float(price)
                price = f"PKR {price_numeric:,.0f}"
            except ValueError:
                price = str(price) # Fallback if float conversion fails unexpectedly
        else:
            price = str(price) # Ensure it's a string for non-numeric cases

        # Build paragraph components
        starter = random.choice(self.paragraph_starters)
        connectors = random.sample(self.paragraph_connectors, k=random.randint(1, 2))
        ending = random.choice(self.paragraph_endings)

        # Create paragraph
        paragraph_parts = [
            f"{starter} a {property_type} property located in {location}.",
            f"The property features {bedrooms} bedrooms and covers {area} square yards of space.",
            f"Priced at {price}, this property offers competitive value in the current market."
        ]

        # Add connector sentences
        for i, connector in enumerate(connectors):
            if i == 0:
                paragraph_parts.append(f"{connector} the {location} area is known for its excellent amenities and secure environment.")
            else:
                paragraph_parts.append(f"{connector} properties in this location tend to appreciate well over time.")

        # Add ending
        paragraph_parts.append(ending)

        # Combine into coherent paragraph
        paragraph = ' '.join(paragraph_parts)

        # Ensure proper punctuation
        sentences = paragraph.split('. ')
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if sent:
                # Capitalize first letter
                sent = sent[0].upper() + sent[1:] if sent else sent
                # Ensure it ends with period
                if not sent.endswith('.'):
                    sent = sent + '.'
                cleaned_sentences.append(sent)

        paragraph = ' '.join(cleaned_sentences)
        return paragraph

    def group_properties_by_location(self, properties: List[dict]) -> Dict[str, List[dict]]:
        """
        Group properties by location for thematic paragraphs
        """
        grouped = defaultdict(list)

        for prop in properties:
            location = prop.get('location', 'Unknown')
            if isinstance(location, str):
                # Clean location name
                location = location.strip().title()
                grouped[location].append(prop)

        return grouped

    def create_location_summary_paragraph(self, location: str, properties: List[dict]) -> str:
        """
        Create summary paragraph for a location with multiple properties
        """
        if not properties:
            return ""

        # Analyze properties in this location
        property_types = set()
        prices = []
        bedrooms_list = []

        for prop in properties:
            if 'property_type' in prop:
                property_types.add(prop['property_type'].lower())

            if 'price' in prop and isinstance(prop['price'], (int, float, str)):
                try:
                    # Safely convert price to float if it's a numeric string
                    price_val = float(prop['price']) if isinstance(prop['price'], str) and prop['price'].replace('.', '', 1).isdigit() else prop['price']
                    if isinstance(price_val, (int, float)): # Ensure it's numeric after potential conversion
                        prices.append(price_val)
                except:
                    pass

            if 'bedrooms' in prop:
                bedrooms_list.append(prop['bedrooms'])

        # Calculate averages
        avg_price = sum(prices) / len(prices) if prices else 0
        common_bedrooms = max(set(bedrooms_list), key=bedrooms_list.count) if bedrooms_list else '3-4'

        # Create summary paragraph
        property_types_str = ', '.join(sorted(property_types)) if property_types else 'various types of'

        paragraph = f"The {location} real estate market offers {property_types_str} properties. "

        if prices:
            paragraph += f"Average prices in this area range around PKR {avg_price:,.0f}, "

        if property_types:
            paragraph += f"with {list(property_types)[0]} properties being particularly popular. "

        paragraph += f"Most properties feature {common_bedrooms} bedrooms, catering to different family sizes. "
        paragraph += f"{location} continues to be a preferred choice for investors and homebuyers alike due to its planned infrastructure and quality of life."

        return paragraph

    def transform_to_paragraphs(self, structured_data: List[dict]) -> List[str]:
        """
        Main method to transform all structured data into training paragraphs
        """
        print("\nTransforming structured data into training paragraphs...")

        all_paragraphs = []

        # Group by location first
        grouped_properties = self.group_properties_by_location(structured_data)

        # 1. Create individual property paragraphs
        print("Creating individual property paragraphs...")
        for prop in structured_data:
            # Create detailed property paragraph
            prop_paragraph = self.create_property_paragraph(prop)
            if prop_paragraph and len(prop_paragraph.split()) >= 30:  # Ensure meaningful length
                all_paragraphs.append(prop_paragraph)

            # Create comprehensive paragraph
            comp_paragraph = self.create_comprehensive_paragraph(prop)
            if comp_paragraph and len(comp_paragraph.split()) >= 40:
                all_paragraphs.append(comp_paragraph)

            # Create Q&A paragraphs
            qa_paragraphs = self.create_qa_paragraph(prop)
            all_paragraphs.extend([p for p in qa_paragraphs if len(p.split()) >= 25])

        # 2. Create location summary paragraphs
        print("Creating location summary paragraphs...")
        for location, props in grouped_properties.items():
            if len(props) >= 2:  # Only create summaries for locations with multiple properties
                summary_paragraph = self.create_location_summary_paragraph(location, props)
                if summary_paragraph and len(summary_paragraph.split()) >= 35:
                    all_paragraphs.append(summary_paragraph)

        # 3. Add general real estate knowledge paragraphs
        print("Adding general knowledge paragraphs...")
        general_paragraphs = self.create_general_knowledge_paragraphs()
        all_paragraphs.extend(general_paragraphs)

        # Remove duplicates while preserving order
        unique_paragraphs = []
        seen = set()
        for para in all_paragraphs:
            para_hash = hashlib.md5(para.encode()).hexdigest()
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)

        print(f"Created {len(unique_paragraphs)} unique training paragraphs")
        print(f"Average paragraph length: {np.mean([len(p.split()) for p in unique_paragraphs]):.1f} words")

        # Show samples
        print("\nSample training paragraphs:")
        for i, para in enumerate(unique_paragraphs[:3], 1):
            print(f"\n{i}. {para[:150]}...")

        return unique_paragraphs

    def create_general_knowledge_paragraphs(self) -> List[str]:
        """
        Create general real estate knowledge paragraphs
        """
        general_paragraphs = [
            "Property investment in Pakistan, particularly in developed areas like DHA, has shown consistent growth over the years. These areas offer planned infrastructure, security, and amenities that attract both local and international investors. Understanding market trends and property values is crucial for making informed investment decisions.",

            "When evaluating a property, consider factors such as location, property type, size, and market conditions. Residential properties in secured communities often appreciate faster due to increasing demand. Commercial properties can offer rental income opportunities in addition to capital appreciation.",

            "The real estate market involves various transaction types including outright purchases, installment plans, and joint ventures. Each approach has its own advantages depending on the investor's financial situation and goals. Proper legal documentation and verification are essential for secure transactions.",

            "Property valuation considers multiple factors including location advantages, construction quality, amenities, and market trends. Professional valuation services can help determine fair market prices. Regular market research helps investors identify emerging opportunities and avoid overpriced properties.",

            "Real estate investment requires careful planning and research. Factors to consider include budget constraints, investment horizon, risk tolerance, and expected returns. Diversifying across different property types and locations can help mitigate risks while maximizing potential returns."
        ]

        return general_paragraphs


class LSTMModelTrainer:
    """
    Train and use LSTM model on paragraph data
    """

    def __init__(self, model_name: str = "paragraph_lstm_model"):
        self.model_name = model_name
        self.tokenizer = Tokenizer(oov_token="<OOV>", filters='')
        self.model = None
        self.max_sequence_len = None
        self.vocab_size = None

    def preprocess_paragraphs(self, paragraphs: List[str]) -> List[str]:
        """
        Clean and prepare paragraph data for training
        """
        processed_paragraphs = []

        for para in paragraphs:
            if not isinstance(para, str):
                continue

            # Ensure paragraph has proper structure
            para = para.strip()

            # Ensure it ends with proper punctuation
            if not para.endswith(('.', '!', '?')):
                para = para + '.'

            # Basic cleaning
            para = para.lower()

            # Remove URLs
            para = re.sub(r'https?://\S+|www\.\S+', '', para)

            # Keep only alphanumeric and basic punctuation
            para = re.sub(r'[^\w\s.,!?;:\'\"-]', '', para)

            # Remove extra whitespace
            para = re.sub(r'\s+', ' ', para).strip()

            # Check paragraph quality - must have multiple sentences
            sentences = re.split(r'[.!?]+', para)
            valid_sentences = [s.strip() for s in sentences if s.strip() and len(s.strip().split()) >= 4]

            if len(valid_sentences) >= 2:  # At least 2 meaningful sentences
                # Reconstruct paragraph with proper spacing
                reconstructed = '. '.join(valid_sentences) + '.'
                reconstructed = re.sub(r'\.\s*\.', '.', reconstructed)  # Remove duplicate periods

                # Ensure minimum word count
                if len(reconstructed.split()) >= 25:
                    processed_paragraphs.append(reconstructed)

        # Remove duplicates while preserving order
        seen = set()
        unique_paragraphs = []
        for para in processed_paragraphs:
            para_hash = hashlib.md5(para.encode()).hexdigest()
            if para_hash not in seen:
                seen.add(para_hash)
                unique_paragraphs.append(para)

        print(f"After preprocessing: {len(unique_paragraphs)} unique paragraphs")
        print(f"Average words per paragraph: {np.mean([len(p.split()) for p in unique_paragraphs]):.1f}")

        # Show sample of processed paragraphs
        print("\nSample processed paragraphs:")
        for i, para in enumerate(unique_paragraphs[:2], 1):
            print(f"\n{i}. {para[:120]}...")

        return unique_paragraphs

    def augment_paragraphs(self, paragraphs: List[str]) -> List[str]:
        """
        Create variations of paragraphs for better training
        """
        augmented_paragraphs = paragraphs.copy()

        for para in paragraphs:
            sentences = re.split(r'[.!?]+', para)
            sentences = [s.strip() for s in sentences if s.strip()]

            if len(sentences) >= 3:
                # Create variations by reordering sentences (keeping some coherence)
                variations = []

                # Variation 1: Move first sentence to end
                if len(sentences) > 2:
                    new_order = sentences[1:] + [sentences[0]]
                    variation = '. '.join(new_order) + '.'
                    variations.append(variation)

                # Variation 2: Reverse sentence order
                if len(sentences) >= 4:
                    new_order = list(reversed(sentences))
                    variation = '. '.join(new_order) + '.'
                    variations.append(variation)

                # Variation 3: Shuffle middle sentences
                if len(sentences) >= 5:
                    middle = sentences[1:-1]
                    random.shuffle(middle)
                    new_order = [sentences[0]] + middle + [sentences[-1]]
                    variation = '. '.join(new_order) + '.'
                    variations.append(variation)

                augmented_paragraphs.extend(variations)

        print(f"Augmented from {len(paragraphs)} to {len(augmented_paragraphs)} paragraphs")
        return augmented_paragraphs

    def prepare_sequences_from_paragraphs(self, paragraphs: List[str], seq_length: int = 100) -> Tuple[np.ndarray, np.ndarray]:
        """
        Convert paragraphs to training sequences
        """
        if not paragraphs:
            raise ValueError("No paragraph data available for training")

        print(f"\nPreparing sequences from paragraphs with target length: {seq_length}")

        # Initialize tokenizer with vocabulary limit
        self.tokenizer = Tokenizer(num_words=15000, oov_token="<OOV>", filters='')

        # Fit tokenizer on all paragraphs
        self.tokenizer.fit_on_texts(paragraphs)
        self.vocab_size = min(len(self.tokenizer.word_index) + 1, 15000)
        print(f"Vocabulary size: {self.vocab_size}")

        # Show most common words
        word_counts = list(self.tokenizer.word_counts.items())
        word_counts.sort(key=lambda x: x[1], reverse=True)
        print(f"Top 10 words: {[word for word, count in word_counts[:10]]}")

        # Create sequences from paragraphs
        sequences = []
        for para in paragraphs:
            token_list = self.tokenizer.texts_to_sequences([para])[0]

            # Skip very short token lists
            if len(token_list) < seq_length:
                continue

            # Generate overlapping sequences from paragraph
            for i in range(seq_length, len(token_list)):
                n_gram_sequence = token_list[i-seq_length:i+1]
                sequences.append(n_gram_sequence)

        if not sequences:
            # Fallback: create shorter sequences for shorter paragraphs
            print("Creating shorter sequences for paragraph training...")
            min_seq_length = 50
            for para in paragraphs:
                token_list = self.tokenizer.texts_to_sequences([para])[0]
                if len(token_list) >= min_seq_length:
                    for i in range(min_seq_length, len(token_list)):
                        n_gram_sequence = token_list[i-min_seq_length:i+1]
                        sequences.append(n_gram_sequence)

        if not sequences:
            raise ValueError("Could not create sequences. Paragraphs might be too short.")

        # Find max sequence length
        self.max_sequence_len = max([len(seq) for seq in sequences])
        print(f"Actual sequence length: {self.max_sequence_len}")
        print(f"Number of sequences created: {len(sequences)}")

        # Pad sequences
        sequences = np.array(pad_sequences(
            sequences,
            maxlen=self.max_sequence_len,
            padding='pre'
        ))

        # Split into input (X) and output (y)
        X = sequences[:, :-1]
        y = sequences[:, -1]

        # Convert y to one-hot encoding
        y = tf.keras.utils.to_categorical(y, num_classes=self.vocab_size)

        print(f"Created {len(X)} training sequences")
        print(f"X shape: {X.shape}, y shape: {y.shape}")
        return X, y

    def build_paragraph_model(self, embedding_dim: int = 300,
                            lstm_units: int = 512,
                            dropout_rate: float = 0.4) -> tf.keras.Model:
        """
        Build LSTM model optimized for paragraph generation
        """
        print("\nBuilding paragraph-optimized LSTM model...")

        model = Sequential([
            # Enhanced embedding layer for better word representation
            Embedding(
                input_dim=self.vocab_size,
                output_dim=embedding_dim,
                input_length=self.max_sequence_len - 1,
                mask_zero=True
            ),

            # First bidirectional LSTM layer
            Bidirectional(LSTM(lstm_units, return_sequences=True)),
            Dropout(dropout_rate),

            # Second LSTM layer for deeper context
            LSTM(lstm_units // 2, return_sequences=True),
            Dropout(dropout_rate),

            # Third LSTM layer for paragraph structure
            LSTM(lstm_units // 4),
            Dropout(dropout_rate / 2),

            # Dense layers for better learning
            Dense(lstm_units // 2, activation='relu'),
            Dropout(dropout_rate / 3),

            Dense(lstm_units // 4, activation='relu'),
            Dropout(dropout_rate / 4),

            # Output layer
            Dense(self.vocab_size, activation='softmax')
        ])

        # Compile model with optimized settings
        model.compile(
            loss='categorical_crossentropy',
            optimizer=Adam(learning_rate=0.0003),  # Lower learning rate for paragraphs
            metrics=['accuracy']
        )

        print("\nParagraph Model Architecture:")
        model.summary()
        self.model = model
        return model

    def train_on_paragraphs(self, X: np.ndarray, y: np.ndarray,
                          epochs: int = 150,  # More epochs for paragraph learning
                          batch_size: int = 128,  # Smaller batch size for paragraphs
                          validation_split: float = 0.15) -> dict:  # More validation data
        """
        Train the model on paragraph data
        """
        print(f"\nStarting paragraph training...")
        print(f"Training samples: {len(X)}")
        print(f"Batch size: {batch_size}")
        print(f"Epochs: {epochs}")

        # Create callbacks with paragraph-specific settings
        callbacks = [
            ModelCheckpoint(
                f'{self.model_name}_paragraph_best.h5',
                monitor='val_accuracy',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=20,  # More patience for paragraph learning
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=0.00001,
                verbose=1
            )
        ]

        # Train with class weight consideration if needed
        history = self.model.fit(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )

        print("\nParagraph training completed!")
        return history.history

    def save_model(self, save_dir: str = "./trained_models"):
        """
        Save the trained model and tokenizer
        """
        os.makedirs(save_dir, exist_ok=True)

        # Save model
        model_path = os.path.join(save_dir, f"{self.model_name}.h5")
        self.model.save(model_path)

        # Save tokenizer
        tokenizer_path = os.path.join(save_dir, f"{self.model_name}_tokenizer.pkl")
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(self.tokenizer, f)

        # Save metadata
        metadata = {
            'vocab_size': self.vocab_size,
            'max_sequence_len': self.max_sequence_len,
            'model_name': self.model_name,
            'word_index': self.tokenizer.word_index
        }

        metadata_path = os.path.join(save_dir, f"{self.model_name}_metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"\nModel saved successfully!")
        print(f"Model: {model_path}")
        print(f"Tokenizer: {tokenizer_path}")
        print(f"Metadata: {metadata_path}")

    def load_model(self, model_dir: str = "./trained_models"):
        """
        Load a saved model
        """
        model_path = os.path.join(model_dir, f"{self.model_name}.h5")
        tokenizer_path = os.path.join(model_dir, f"{self.model_name}_tokenizer.pkl")
        metadata_path = os.path.join(model_dir, f"{self.model_name}_metadata.json")

        # Check if files exist
        for path in [model_path, tokenizer_path, metadata_path]:
            if not os.path.exists(path):
                raise FileNotFoundError(f"File not found: {path}")

        # Load model
        self.model = load_model(model_path)

        # Load tokenizer
        with open(tokenizer_path, 'rb') as f:
            self.tokenizer = pickle.load(f)

        # Load metadata
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
            self.vocab_size = metadata['vocab_size']
            self.max_sequence_len = metadata['max_sequence_len']

        print(f"Model loaded from {model_path}")
        print(f"Vocabulary size: {self.vocab_size}")

    def generate_paragraph(self, seed_text: str,
                         num_words: int = 150,  # Generate longer text for paragraphs
                         temperature: float = 0.7,
                         max_attempts: int = 3) -> str:
        """
        Generate paragraph from seed text
        """
        if self.model is None:
            raise ValueError("Model not loaded. Train or load a model first.")

        generated_text = seed_text
        seed_text_original = seed_text

        # Ensure seed text is properly formatted
        seed_text = seed_text.lower().strip()
        if not seed_text.endswith(('.', '!', '?')):
            seed_text = seed_text + '.'

        for word_num in range(num_words):
            for attempt in range(max_attempts):
                try:
                    # Tokenize and pad
                    token_list = self.tokenizer.texts_to_sequences([seed_text])[0]
                    if len(token_list) < self.max_sequence_len - 1:
                        padding_needed = self.max_sequence_len - 1 - len(token_list)
                        token_list = [0] * padding_needed + token_list

                    token_list = token_list[-(self.max_sequence_len - 1):]  # Take last n tokens
                    token_list = np.array(token_list).reshape(1, -1)

                    # Predict
                    predictions = self.model.predict(token_list, verbose=0)[0]

                    # Apply temperature with smoothing
                    if temperature > 0:
                        predictions = np.log(predictions + 1e-10) / temperature
                        exp_predictions = np.exp(predictions)
                        predictions = exp_predictions / np.sum(exp_predictions)

                    # Filter very low probability predictions
                    predictions = np.where(predictions < 0.001, 0, predictions)
                    predictions = predictions / np.sum(predictions)  # Renormalize

                    # Sample next word
                    predicted_id = np.random.choice(len(predictions), p=predictions)

                    # Convert to word
                    output_word = ""
                    for word, idx in self.tokenizer.word_index.items():
                        if idx == predicted_id:
                            output_word = word
                            break

                    # Update texts
                    if output_word:
                        seed_text += " " + output_word
                        generated_text += " " + output_word

                    # Check for natural paragraph breaks
                    if output_word and output_word.endswith(('.', '!', '?')):
                        if len(generated_text.split()) > 50:  # Minimum paragraph length
                            break

                    break

                except Exception as e:
                    if attempt == max_attempts - 1:
                        print(f"Failed to generate word {word_num + 1}: {e}")
                        return generated_text

        # Post-process for paragraph coherence
        generated_text = self._post_process_paragraph(generated_text, seed_text_original)

        return generated_text

    def _post_process_paragraph(self, text: str, original_prompt: str) -> str:
        """
        Clean and format generated paragraph for better readability
        """
        # Extract only the generated part
        if text.startswith(original_prompt):
            response = text[len(original_prompt):].strip()
        else:
            response = text

        # Split into sentences
        sentences = re.split(r'[.!?]+', response)

        # Filter and clean sentences
        clean_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if sent:
                words = sent.split()
                # Keep sentences of reasonable length and coherence
                if 5 <= len(words) <= 40:
                    # Check for basic sentence structure
                    if any(word in sent.lower() for word in ['is', 'are', 'has', 'have', 'was', 'were', 'will', 'can']):
                        # Capitalize first letter
                        sent = sent[0].upper() + sent[1:] if sent else sent

                        # Add proper ending punctuation
                        if not sent.endswith(('.', '!', '?')):
                            sent = sent + '.'

                        clean_sentences.append(sent)

        # Combine into coherent paragraph (3-5 sentences ideal)
        if clean_sentences:
            paragraph_sentences = clean_sentences[:min(5, len(clean_sentences))]
            response = ' '.join(paragraph_sentences)
        else:
            # Fallback response
            response = "Based on the available property data, I can provide information about various real estate opportunities. The market offers different property types in various locations with competitive pricing. Investors should consider factors like location, property type, and market trends."

        # Ensure proper spacing
        response = re.sub(r'\s+', ' ', response).strip()

        return response

    def get_paragraph_response(self, prompt: str,
                             max_length: int = 120,
                             temperature: float = 0.65) -> str:
        """
        Get AI paragraph response for a prompt
        """
        # Clean and format prompt
        prompt = prompt.lower().strip()
        prompt = re.sub(r'[^\w\s.,!?]', '', prompt)

        # Ensure prompt ends with proper punctuation for better generation
        if not prompt.endswith(('.', '!', '?')):
            prompt = prompt + '?'

        print(f"Generating paragraph response for: '{prompt}'...")

        # Generate paragraph response
        full_response = self.generate_paragraph(
            seed_text=prompt,
            num_words=max_length,
            temperature=temperature
        )

        # Extract and clean the response
        response = full_response[len(prompt):].strip()

        # Format response as proper paragraph
        if response:
            # Capitalize first letter
            response = response[0].upper() + response[1:] if response else response

            # Ensure it ends with proper punctuation
            if response and response[-1] not in '.!?':
                response = response + '.'

            # Check paragraph length
            words = response.split()
            if len(words) > max_length:
                # Find a good cutoff point (end of sentence)
                truncated = ' '.join(words[:max_length])
                # Try to end at sentence boundary
                last_period = truncated.rfind('.')
                last_question = truncated.rfind('?')
                last_exclamation = truncated.rfind('!')

                cutoff = max(last_period, last_question, last_exclamation)
                if cutoff > max_length * 0.5:  # If we found a reasonable sentence end
                    response = truncated[:cutoff + 1]
                else:
                    response = truncated + '...'

        return response


class APIAIModel:
    """
    Main class to manage API data fetching, paragraph creation, and model training
    """

    def __init__(self):
        self.data_fetcher = APIDataFetcher(max_workers=5)
        self.paragraph_creator = ParagraphCreator()
        self.model_trainer = None
        self.api_configs = []
        self.training_paragraphs = []

    def add_api_endpoint(self, url: str, method: str = 'GET',
                         params: dict = None, headers: dict = None,
                         data: dict = None, name: str = None):
        """
        Add an API endpoint to fetch data from
        """
        if not name:
            # Generate name from URL
            parsed = urlparse(url)
            name = f"{parsed.netloc}{parsed.path.replace('/', '_')}"

        api_config = {
            'name': name,
            'url': url,
            'method': method,
            'params': params or {},
            'headers': headers or {},
            'data': data,
            'timeout': 30
        }

        self.api_configs.append(api_config)
        print(f"Added API: {name} - {url}")

    def add_multiple_apis(self, apis: List[dict]):
        """
        Add multiple APIs at once
        """
        for api in apis:
            self.add_api_endpoint(**api)

    def fetch_and_create_paragraphs(self) -> List[str]:
        """
        Fetch data from APIs and transform into training paragraphs
        """
        if not self.api_configs:
            print("No APIs added. Please add API endpoints first.")
            return []

        print(f"\nFetching data from {len(self.api_configs)} APIs...")

        # Fetch structured data from APIs
        structured_data = self.data_fetcher.fetch_multiple_apis(self.api_configs)

        if not structured_data:
            print("No structured data fetched from APIs.")
            return []

        print(f"\nFetched {len(structured_data)} structured data items")

        # Transform structured data into paragraphs
        print("\nCreating training paragraphs from API data...")
        paragraphs = self.paragraph_creator.transform_to_paragraphs(structured_data)

        # Store for later use
        self.training_paragraphs = paragraphs

        # Save paragraphs for reference
        if paragraphs:
            os.makedirs("./api_data", exist_ok=True)

            # Save raw paragraphs
            with open("./api_data/training_paragraphs.json", 'w', encoding='utf-8') as f:
                json.dump(paragraphs, f, indent=2, ensure_ascii=False)

            # Save paragraph statistics
            stats = {
                'total_paragraphs': len(paragraphs),
                'avg_words_per_paragraph': np.mean([len(p.split()) for p in paragraphs]),
                'total_words': sum([len(p.split()) for p in paragraphs]),
                'sample_paragraphs': paragraphs[:3]
            }

            with open("./api_data/paragraph_stats.json", 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)

            print(f"\nParagraph data saved:")
            print(f"  - Training paragraphs: ./api_data/training_paragraphs.json")
            print(f"  - Statistics: ./api_data/paragraph_stats.json")
            print(f"\nCreated {len(paragraphs)} training paragraphs")

        return paragraphs

    def train_model_on_paragraphs(self, model_name: str = "paragraph_trained_model",
                                seq_length: int = 80, epochs: int = 150,
                                batch_size: int = 128, test_size: float = 0.15):
        """
        Train LSTM model on paragraph data
        """
        if not self.training_paragraphs:
            print("No training paragraphs available. Please fetch data and create paragraphs first.")
            return False

        print("\n" + "="*70)
        print("TRAINING LSTM MODEL ON PARAGRAPH DATA")
        print("="*70)
        print("Training model to generate coherent English paragraphs...")

        # Initialize trainer
        self.model_trainer = LSTMModelTrainer(model_name=model_name)

        # Preprocess paragraphs
        print("\n1. Preprocessing paragraph data...")
        processed_paragraphs = self.model_trainer.preprocess_paragraphs(self.training_paragraphs)

        if not processed_paragraphs:
            print("No valid paragraph data after preprocessing.")
            return False

        # Augment paragraph data
        print("\n2. Augmenting paragraph data...")
        augmented_paragraphs = self.model_trainer.augment_paragraphs(processed_paragraphs)
        print(f"Paragraphs augmented from {len(processed_paragraphs)} to {len(augmented_paragraphs)}")

        # Prepare sequences from paragraphs
        print("\n3. Preparing training sequences from paragraphs...")
        try:
            X, y = self.model_trainer.prepare_sequences_from_paragraphs(augmented_paragraphs, seq_length)
        except ValueError as e:
            print(f"Error preparing sequences: {e}")
            return False

        # Build paragraph-optimized model
        print("\n4. Building paragraph-optimized LSTM model...")
        self.model_trainer.build_paragraph_model(
            embedding_dim=300,
            lstm_units=512,
            dropout_rate=0.4
        )

        # Train model on paragraphs
        print("\n5. Training model on paragraph data...")
        history = self.model_trainer.train_on_paragraphs(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=test_size
        )

        # Save model
        print("\n6. Saving trained paragraph model...")
        self.model_trainer.save_model()

        # Display results
        print("\n" + "="*70)
        print("PARAGRAPH TRAINING COMPLETED!")
        print("="*70)
        if 'val_accuracy' in history:
            print(f"Final Validation Accuracy: {history['val_accuracy'][-1]:.2%}")
        if 'val_loss' in history:
            print(f"Final Validation Loss: {history['val_loss'][-1]:.4f}")

        # Calculate and display additional metrics
        if 'accuracy' in history:
            train_acc = history['accuracy'][-1]
            print(f"Final Training Accuracy: {train_acc:.2%}")

        print(f"\nModel is now trained to generate coherent paragraphs.")

        return True

    def load_trained_model(self, model_name: str = "paragraph_trained_model"):
        """
        Load a previously trained model
        """
        try:
            self.model_trainer = LSTMModelTrainer(model_name=model_name)
            self.model_trainer.load_model()
            print(f"Model '{model_name}' loaded successfully!")
            return True
        except Exception as e:
            print(f"Error loading model: {e}")
            return False

    def test_paragraph_model(self, test_prompts: List[str] = None):
        """
        Test the trained paragraph model
        """
        if self.model_trainer is None or self.model_trainer.model is None:
            print("Model not loaded. Please train or load a model first.")
            return

        if test_prompts is None:
            # Paragraph-focused test prompts
            test_prompts = [
                "What is the property rate at DHA Phase 5?",
                "Can you describe available properties in DHA?",
                "Tell me about commercial property investment opportunities.",
                "What factors affect property prices in developed areas?",
                "How does location impact property value?",
                "What should I consider when buying property?",
                "Describe the real estate market in Pakistan.",
                "What are the benefits of investing in DHA properties?"
            ]

        print("\n" + "="*70)
        print("PARAGRAPH MODEL TESTING")
        print("="*70)
        print("Testing model's ability to generate coherent paragraphs...")

        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}. Prompt: {prompt}")
            try:
                response = self.model_trainer.get_paragraph_response(prompt, max_length=120, temperature=0.65)
                print(f"\n   Response: {response}")

                # Show response metrics
                words = response.split()
                sentences = len(re.split(r'[.!?]+', response)) - 1
                print(f"   [Words: {len(words)}, Sentences: {sentences}]")
            except Exception as e:
                print(f"   Error: {e}")
            print("-"*70)

    def interactive_paragraph_chat(self):
        """
        Interactive chat with paragraph generation
        """
        if self.model_trainer is None or self.model_trainer.model is None:
            print("Model not loaded. Please train or load a model first.")
            return

        print("\n" + "="*70)
        print("PARAGRAPH CHAT INTERFACE")
        print("="*70)
        print("Ask questions and receive paragraph-length responses")
        print("Commands:")
        print("  'quit' or 'exit' - End chat")
        print("  'temp X' - Set temperature (0.1-1.0, default: 0.65)")
        print("  'len X' - Set response length (default: 100 words)")
        print("="*70)

        temperature = 0.65
        response_length = 100

        while True:
            try:
                user_input = input("\nYou: ").strip()

                if not user_input:
                    continue

                # Check for commands
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("Goodbye!")
                    break

                elif user_input.lower().startswith('temp '):
                    try:
                        new_temp = float(user_input.split()[1])
                        if 0.1 <= new_temp <= 1.0:
                            temperature = new_temp
                            print(f"Temperature set to {temperature}")
                        else:
                            print("Temperature must be between 0.1 and 1.0")
                    except:
                        print("Invalid format. Use: temp 0.65")
                    continue

                elif user_input.lower().startswith('len '):
                    try:
                        new_len = int(user_input.split()[1])
                        if 50 <= new_len <= 200:
                            response_length = new_len
                            print(f"Response length set to {response_length} words")
                        else:
                            print("Length must be between 50 and 200 words")
                    except:
                        print("Invalid format. Use: len 100")
                    continue

                # Get AI paragraph response
                start_time = time.time()
                response = self.model_trainer.get_paragraph_response(
                    prompt=user_input,
                    max_length=response_length,
                    temperature=temperature
                )
                response_time = time.time() - start_time

                # Calculate response metrics
                words = response.split()
                sentences = len(re.split(r'[.!?]+', response)) - 1

                print(f"\nAI: {response}")
                print(f"[{len(words)} words, {sentences} sentences, {response_time:.2f}s, temp={temperature}]")

            except KeyboardInterrupt:
                print("\n\nChat ended by user.")
                break
            except Exception as e:
                print(f"Error: {e}")
                print("AI: I apologize, I encountered an error while generating a response. Please try again.")

    def view_paragraph_statistics(self):
        """
        View statistics about training paragraphs
        """
        if not self.training_paragraphs:
            print("No training paragraphs available.")
            return

        print("\n" + "="*70)
        print("TRAINING PARAGRAPH STATISTICS")
        print("="*70)

        paragraphs = self.training_paragraphs
        word_counts = [len(p.split()) for p in paragraphs]
        char_counts = [len(p) for p in paragraphs]

        print(f"Total Paragraphs: {len(paragraphs)}")
        print(f"Average Words per Paragraph: {np.mean(word_counts):.1f}")
        print(f"Average Characters per Paragraph: {np.mean(char_counts):.1f}")
        print(f"Minimum Words: {min(word_counts)}")
        print(f"Maximum Words: {max(word_counts)}")
        print(f"Total Words: {sum(word_counts)}")

        total_sentences = 0
        for para in paragraphs:
            sentences = re.split(r'[.!?]+', para)
            total_sentences += len([s for s in sentences if s.strip()])

        print(f"Total Sentences: {total_sentences}")
        print(f"Average Sentences per Paragraph: {total_sentences/len(paragraphs):.1f}")

        print("\nSample Paragraphs:")
        for i, para in enumerate(paragraphs[:3], 1):
            sentences = re.split(r'[.!?]+', para)
            valid_sentences = [s.strip() for s in sentences if s.strip()]
            print(f"\n{i}. Words: {len(para.split())}, Sentences: {len(valid_sentences)}")
            print(f"   Preview: {para[:100]}...")


def create_api_configs_from_urls(api_urls: List[str]) -> List[dict]:
    """
    Helper function to create API configs from simple URLs
    """
    configs = []
    for i, url in enumerate(api_urls):
        configs.append({
            'url': url,
            'method': 'GET',
            'name': f'api_{i+1}'
        })
    return configs


def main():
    """
    Main execution function for paragraph-based training
    """
    print("="*80)
    print("PARAGRAPH-BASED LSTM AI MODEL TRAINED ON API DATA")
    print("="*80)
    print("This version transforms API data into coherent paragraphs")
    print("and trains the model to generate proper English responses.")
    print("="*80)

    # Initialize the system
    ai_system = APIAIModel()

    while True:
        print("\n" + "="*80)
        print("PARAGRAPH TRAINING MENU")
        print("="*80)
        print("1. Add API endpoints")
        print("2. Fetch data & Create paragraphs")
        print("3. Train model on paragraphs")
        print("4. Load existing paragraph model")
        print("5. Test paragraph model")
        print("6. Interactive paragraph chat")
        print("7. View paragraph statistics")
        print("8. Exit")

        choice = input("\nEnter choice (1-8): ").strip()

        if choice == '1':
            print("\nAdd API Endpoints")
            print("-" * 40)
            print("Enter API URLs (one per line, empty line to finish):")

            api_urls = []
            while True:
                url = input("API URL: ").strip()
                if not url:
                    break
                if url.startswith('http'):
                    api_urls.append(url)
                    print(f"Added: {url}")
                else:
                    print("Invalid URL. Must start with http:// or https://")

            if api_urls:
                configs = create_api_configs_from_urls(api_urls)
                ai_system.add_multiple_apis(configs)
                print(f"\nAdded {len(api_urls)} API endpoints.")

        elif choice == '2':
            if not ai_system.api_configs:
                print("No APIs added. Please add API endpoints first.")
            else:
                print(f"\nFetching data from {len(ai_system.api_configs)} APIs...")
                paragraphs = ai_system.fetch_and_create_paragraphs()
                if paragraphs:
                    print(f"\nSuccessfully created {len(paragraphs)} training paragraphs.")
                    print("You can now train the model on these paragraphs.")

        elif choice == '3':
            if not ai_system.training_paragraphs:
                print("No training paragraphs. Please fetch data and create paragraphs first.")
                continue

            print("\nTrain Paragraph Model")
            print("-" * 40)

            model_name = input("Model name [paragraph_model]: ").strip() or "paragraph_model"
            seq_length = input("Sequence length [80]: ").strip()
            seq_length = int(seq_length) if seq_length else 80
            epochs = input("Training epochs [150]: ").strip()
            epochs = int(epochs) if epochs else 150
            batch_size = input("Batch size [128]: ").strip()
            batch_size = int(batch_size) if batch_size else 128

            success = ai_system.train_model_on_paragraphs(
                model_name=model_name,
                seq_length=seq_length,
                epochs=epochs,
                batch_size=batch_size
            )

            if success:
                print("\nParagraph model trained successfully!")

        elif choice == '4':
            model_name = input("Enter model name to load [paragraph_trained_model]: ").strip()
            model_name = model_name or "paragraph_trained_model"

            if ai_system.load_trained_model(model_name):
                print("Paragraph model loaded successfully!")

        elif choice == '5':
            ai_system.test_paragraph_model()

        elif choice == '6':
            ai_system.interactive_paragraph_chat()

        elif choice == '7':
            ai_system.view_paragraph_statistics()

        elif choice == '8':
            print("\nThank you for using the Paragraph-based API AI Model!")
            break

        else:
            print("Invalid choice. Please enter 1-8.")


def quick_start_example():
    """
    Quick start with example API URLs for paragraph training
    """
    print("\n" + "="*80)
    print("PARAGRAPH TRAINING QUICK START")
    print("="*80)

    # YOUR API URLs GO HERE - REPLACE WITH YOUR ACTUAL APIS
    your_api_urls = [
        # Add your actual API URLs here:
        "https://api.dhaconnects.com/api/property",
    ]

    print("\nUsing example API. Replace with your actual API URLs in the code.")
    print(f"Number of APIs: {len(your_api_urls)}")

    # Create the AI system
    ai_system = APIAIModel()

    # Add APIs
    configs = create_api_configs_from_urls(your_api_urls)
    ai_system.add_multiple_apis(configs)

    # Fetch data and create paragraphs
    print("\nFetching data and creating training paragraphs...")
    ai_system.fetch_and_create_paragraphs()

    if not ai_system.training_paragraphs:
        print("No paragraphs created. Check your API URLs and internet connection.")
        return

    # Train model on paragraphs
    print("\nTraining paragraph model...")
    success = ai_system.train_model_on_paragraphs(
        model_name="paragraph_api_ai",
        seq_length=80,
        epochs=150,
        batch_size=128
    )

    if success:
        # Test the model
        ai_system.test_paragraph_model()

        # Optional: Start interactive chat
        chat = input("\nStart interactive paragraph chat? (yes/no): ").strip().lower()
        if chat in ['yes', 'y']:
            ai_system.interactive_paragraph_chat()


if __name__ == "__main__":
    # Run the main program
    main()

    # Or run quick start (comment out main() above first)
    # quick_start_example()